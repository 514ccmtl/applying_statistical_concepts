{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the importlib module for reloading modules\n",
    "import importlib\n",
    "\n",
    "# Import the helper functions from the exercise_0_helper_functions module\n",
    "import helper_functions\n",
    "\n",
    "# Reload the exercise_0_helper_functions module to ensure we have the latest version\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "# Import all functions from the reloaded exercise_0_helper_functions module\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Title: Breast Cancer Classification with Regression\n",
    "\n",
    "## Introduction\n",
    "Welcome to Exercise 1! In this notebook, we will go through the importance of applying statistical concepts to our data problems and how to get meaningful answers to our questions using the power of programming. This module will help you understand many things about your data.\n",
    "\n",
    "In the world of statistics, we use a tool called **regression** to help us answer such questions. Regression is a method used to find how one thing (like temperature) can predict another thing (like ice cream sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temperature_vs_ice_cream_sales()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use regression to classify things, like determining if a student will pass or fail an exam based on their study hours and sleep hours. If you look at the graph below, you can see that the blue line separates the data points. If you fall on the left side of the line, there is a good chance you will fail, and if you land on the right side of the line, there is a good chance you will pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_and_study()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to find patterns in your data and learning from it is powerful. Yes, ice cream sales and exam results may be low stakes, but regression can be used in more critical environments.\n",
    "\n",
    "Imagine you have a big source of cancer data at your disposal. You want to see if there is a pattern among them to better aid future patients. This is a big deal, as it allows you to better take care of patients and essentially buy more time by intervening sooner.\n",
    "\n",
    "Wouldn't it be nice to answer some key questions? For example, how big does a tumor have to be, to be considered cancerous? How smooth or rough must the texture be for it to be cancerous? \n",
    "\n",
    "In this exercise, we will go through breast cancer data to answer our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the Dataset\n",
    "In this notebook, we will explore the Breast Cancer dataset, use a Regression method to classify data, and evaluate the model's performance. \n",
    "\n",
    "First, we load the Breast Cancer dataset using the `load_breast_cancer()` function from the scikit-learn library. This dataset is a well-known collection of breast cancer data that includes various measurements and features related to tumor characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# Convert the dataset into a DataFrame for visualization\n",
    "df = pd.DataFrame(data=np.c_[X, y], columns=np.append(breast_cancer.feature_names, 'target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded into variables `X` for the measurements and `y` for the target labels (cancerous or not cancerous). For easier visualization and manipulation, we convert the dataset into a Pandas DataFrame, combining the measurements and target labels into one structured table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploring the Dataset\n",
    "Next, let's take a quick look at the dataset to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the table you created represents an individual sample (or patient) from the breast cancer dataset. The columns correspond to various features measured or calculated for that sample, except for the last column labeled “target,” which indicates the classification of the sample (0 indicates a benign tumor, which is not cancerous, and 1 indicates a malignant tumor, which is cancerous.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary statistics of the dataset\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of target classes\n",
    "plot_target_class_distribution(df, breast_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Breast Cancer dataset includes a large number of measurements related to tumor characteristics, making it challenging to visualize all of them at once. \n",
    "\n",
    "To simplify our task and create a clear visualization, we decided to use the first two columns in the dataset: _\"average radius\"_ and _\"average texture\"_. This choice is made for convenience, allowing us to quickly build and visualize our model. Focusing on just these two measurements makes the visualization clearer and more interpretable while still providing meaningful insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with that in mind lets draw a scatter plot of our data to see things better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the initial data points\n",
    "scatter_plot_two_features(X, \"mean_radius\", \"mean_texture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the benign and malignant labels to color each data point, we can see how average radius and average texture (or mean radius and mean texture) affect (or don't affect) the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the color-coded data points (with colors indicating benign and malignant tumors)\n",
    "scatter_plot_with_class_labels(X, y, \"mean radius\", \"mean texture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can see our data visually, we notice that there may be a way to classify the benign and malignant cases based on mean radius and mean texture. As humans, we can visually observe potential patterns in the data points. To formalize this observation, we will use regression as a tool to help us classify these cases. The next step is to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Splitting the Dataset\n",
    "Now, we will split the dataset into **training** and **testing** sets for model training and evaluation. This is important because it allows us to train the model on one part of the data and test it on another part to see how well it performs on new data. This helps us understand the model's ability to make accurate predictions on data it hasn't seen before. We will learn more about this process and its significance later on in the module.\n",
    "\n",
    "In our example today, we will use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# random_state=42 ensures the split is reproducible, meaning the data is split the same way each time the code is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building and Training a Classification Model\n",
    "Next, we will build and train a regression model for breast cancer classification. Specifically, we will use a **logistic regression** model that classifies the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Select specific variables (mean radius and mean texture) for training\n",
    "X_train_selected = X_train[:, [0, 1]]  # Columns 0 and 1 correspond to mean radius and mean texture\n",
    "\n",
    "# Train the model on the selected variables of the training data\n",
    "model.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if everything went well, we should have successfully initialized and trained our logistic regression model using the selected variables (mean radius and mean texture). The model has learned from the training data and is now ready to make predictions. In the next step, we will evaluate the performance of our model to see how well it can classify new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the Model\n",
    "Now, let's evaluate the performance of the trained model on the testing data.\n",
    "\n",
    "We will use the `model.predict` method to make predictions on the testing data. This method takes the testing data as input and outputs the predicted classifications for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "y_pred = model.predict(X_train_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate it using something called a **confusion matrix**. A confusion matrix allows us to see how well our model is performing by comparing the predicted classifications to the actual classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plot_confusion_matrix(cm, breast_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also calculate its **accuracy** and generate a **classification report**.\n",
    "\n",
    "First, we calculate the accuracy of the model using the `accuracy_score` function. This function compares the true labels (`y_train`) with the predicted labels (`y_pred`) and returns the percentage of correct predictions. The accuracy score provides a simple metric to understand how often the model makes correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model Accuracy\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a classification report using the `print_classification_report` function. This report provides a more detailed evaluation of the model, including precision, recall, and F1-score for each class. These metrics help us understand the performance of the model beyond just accuracy, giving insight into how well the model identifies each class. You don’t need to know the details of these metrics right now, but it’s good to be aware of them. They may come in handy down the road, so consider reading more about them when you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report\n",
    "print_classification_report(y_train, y_pred, breast_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualizing the Decision Boundary\n",
    "\n",
    "Now that we see the accuracy is satisfactory, we will take the next step to visualize our model on the graph. By plotting the decision boundary of the logistic regression model, we can better understand how the model classifies the data points and observe the separation between benign and malignant cases. This visualization will help us **see** the effectiveness of our model in distinguishing between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary along with the color-coded data points\n",
    "scatter_plot_with_decision_boundary(X, y, model, \"mean radius\", \"mean texture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see why the accuracy wasn't 100% by looking at the graph. The data points are not perfectly separable by a linear line, indicating that there is some overlap between the benign and malignant cases. This overlap means that a simple linear model will never be able to achieve 100% accuracy with this dataset. The complexity and nature of the data make it challenging for a linear boundary to perfectly classify all instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat all of the steps above **using different columns**, the following Python cell will randomly select two columns and run logistic regression on them to classify the data. Each time you run the cell, it will select different columns for the classification, and you will observe that the accuracy changes accordingly. This is because different variables have varying degrees of correlation with the target variable, impacting the model's performance.\n",
    "\n",
    "run `run_it_all_over()` many times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_it_all_over()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circling back to the real-world application of our work, this model has the potential to save lives. By using past data to train the model, we can now quickly and accurately classify new patients as having benign or malignant tumors based on their data. This means that if a new patient's data falls on one side of the decision boundary, with the accuracy of the model in mind, we can quickly determine their diagnosis and take appropriate action. This ability to rapidly and accurately diagnose breast cancer can lead to earlier interventions, better treatment plans, and ultimately, improved patient outcomes.\n",
    "\n",
    "run `help_me_in_real_life()` many times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_me_in_real_life()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Regression is a method for modeling the relationship between a dependent variable and one or more independent variables. By leveraging past data, regression helps us find patterns and make predictions about future outcomes.\n",
    "\n",
    "With the power of programming, regression becomes an even more potent tool in the hands of data scientists. It allows us to automate the analysis of large datasets, quickly build models, and visualize complex relationships. This enables us to gain insights and make data-driven decisions more efficiently.\n",
    "\n",
    "However, there is always room for improvement. While our current model uses a simple logistic regression approach, we can explore more advanced techniques. For instance, we can use polynomial regression to fit curves rather than straight lines, or employ regularization methods to enhance model performance. Additionally, there are many ways to refine the models we've created, such as incorporating more variables, and using cross-validation to ensure robustness.\n",
    "\n",
    "As we continue to learn and apply these advanced techniques, we can build more accurate and reliable models, ultimately leading to better predictions and more informed decisions in various fields, including healthcare, finance, and beyond.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
