---
title: "6.2: Linear Regression"
author: "Navona Calarco"
format:
  beamer:
    aspectratio: 169
    theme: Madrid
    colortheme: DarkBlue
    institute: The University of Toronto
    # toc: true
    # Table of contents not generating at the moment
editor: visual
header-includes: 
- \definecolor{DarkBlue}{rgb}{0.05, 0.15, 0.3}
- \setbeamercolor{structure}{fg=DarkBlue}
---

# Motivation

Throughout this Module we will be making use of the `Boston` dataset in the R package `ISLR2`. We can install the package in R and add it to our library:

```         
install.packages("ISLR2")
library(ISLR2)
```

The `Boston` dataset contains housing values in 506 Boston suburbs along with 12 other variables associated with the suburbs. To name a few,

-   `rm`: average number of rooms per dwelling
-   `nox`: nitrogen oxides concentration (parts per 10 million)
-   `lstat`: percent of households with low socioeconomics status

We can take `medv`, the median value of owner-occupied homes in \$1000s, to be the response variable $Y$ and the 12 other variables to be the predictors $X = (X_1, \dots, X_{12})$.

------------------------------------------------------------------------

# Motivation

There may be some specific question we'd like to address

-   Is there a relationship between the 12 variables and housing price?
    -   Does the data provide evidence of an association?
-   Are all of the 12 variables associated with housing price?
    -   Perhaps only a few of the variables have an effect on housing price.
-   How accurate are the predictions for housing prices based on these variables?
-   Is the relationship between the variables and housing price linear?
    -   Perhaps we can transform some variables to make the relationship linear.

\alert{All of these questions can be answered using linear regression!}

------------------------------------------------------------------------

# Simple Linear Regression

**Simple linear regression** uses a *single* predictor variable $X$ to predict a *quantitative* response $Y$ by assuming the relationship between them is linear. $$Y \approx \beta_0 + \beta_1 X$$

-   $\beta_0$ and $\beta_1$ are the model **parameters** which are unknown.
-   $\beta_0$ is the intercept term and $\beta_1$ is the slope term.

We can use the training data to produce estimates $\hat \beta_0$ and $\hat \beta_1$ and predict future responses

$$\hat{y} \approx \hat{\beta_0} + \hat{\beta_1} X$$

------------------------------------------------------------------------

# Estimating the Coefficients

Suppose we have $n$ observations in our training data which each consists of a measurement for $X$ and $Y$ represented by $$
(x_1, y_1),\ (x_2, y_2), \dots,\ (x_n, y_n)
$$

We want to find estimates for $\hat \beta_0$ and $\hat \beta_1$ such that for all $i = 1, \dots, n$ $$
y_i \approx \hat y_i
$$

where $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$ is the prediction for $y_i$ given $x_i$.

The most common method used to measure the difference between $y_i$ and $\hat y_i$ is the least squares criterion. The idea being that \alert{we want to find the $\hat \beta_0$ and $\hat \beta_1$ that give us the smallest difference}.

------------------------------------------------------------------------

# Least Squares Criterion

We define the $i$th **residual** to be the difference between the $i$th observed response value and the $i$th predicted response value: $$
e_i = y_i - \hat y_i
$$

The **residual sum of squares** (RSS) is the following $$
\operatorname{RSS} = e_1^2 + \cdots + e_n^2 
= \left(y_{1}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{1}\right)^{2}+\cdots+\left(y_{n}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{n}\right)^{2}
$$

### The RSS is minimized by the estimates below (where $\bar{x},\ \bar{y}$ are the sample means):

$$
\begin{aligned}
\hat{\beta}_{1}&=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\
\hat{\beta}_{0}&=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{aligned}
$$ \alert{So $\hat \beta_1$ and $\hat \beta_0$ definte the least squares coefficient estimates}

------------------------------------------------------------------------

# Assessing the Accuracy of the Coefficient Estimates

Recall from section 6.1 that we assume the true relationship between the predictor $X$ and the response $Y$ is $$
Y = f(X) + \epsilon
$$ where $f$ is an unknown function and $\epsilon$ is the random error with mean zero. By assuming $f$ is linear, we obtain $$
Y = \beta_0 + \beta_1 X + \epsilon
$$ Now suppose we have the least squares coefficient estimates $\hat \beta_0$ and $\hat \beta_1$, so $$
\hat Y = \hat \beta_0 + \hat \beta_1 X
$$ We would like to assess the how close $\hat \beta_0$ and $\hat \beta_1$ are to the true parameter values $\beta_0$ and $\beta_1$.

------------------------------------------------------------------------

# Standard Error

We can compute the **standard erorrs** associated with $\hat \beta_0$ and $\hat \beta_1$ with the following: $$
\operatorname{SE}\left(\hat{\beta}_{0}\right)^{2}=\sigma^{2}\left[\frac{1}{n}+\frac{\bar{x}^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\right], \quad \quad  \operatorname{SE}\left(\hat{\beta}_{1}\right)^{2}=\frac{\sigma^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
$$ where $\sigma^2 = \operatorname{Var}(\epsilon)$ and is usually unknown. Luckily, $\sigma$ can be estimated from the data using the **residual standard error** (RSE) $$
\operatorname{RSE} = \sqrt{\frac{\operatorname{RSS}}{(n-2)}}
$$ The standard errors for $\hat \beta_0$ and $\hat \beta_1$ can be used to compute confidence intervals of the estimates or perform hypothesis tests on the coefficients.

------------------------------------------------------------------------

# Hypothesis Tests on the Coefficients

Once we have the standard errors, we can perform a hypothesis test on the coefficients to determine whether there is a relationship between $X$ and $Y$. The null hypothesis is $$
H_0: \text{ There is no relationship between } X \text{ and } Y
$$ and the alternative hypothesis is $$
H_{a}: \text { There is some relationship between } X \text { and } Y
$$

Mathematically, this is $$
H_0: \beta_1 = 0 \quad \text{versus} \quad H_a: \beta_1 \neq 0
$$ since if $\beta_1 = 0$ then $Y = \beta_0 + \epsilon$ so $Y$ is not associated with $X$.

------------------------------------------------------------------------

# Hypothesis Tests on the Coefficients

In order to test the null hypothesis, we need to determine whether $\hat \beta_1$ is sufficiently far from zero. The $\mathbf{t}$**-statistic** $$
t=\frac{\hat{\beta}_{1}-0}{\operatorname{SE}(\hat{\beta}_{1})}
$$ measures the number of standard deviations that $\hat \beta_1$ is away from 0. The $p$-value can be computed from the $t$-statistic which will allow us to either accept or reject our null hypothesis.

------------------------------------------------------------------------

# Assessing the Accuracy of the Model

The quality of the linear regression fit is often assessed with the residual standard error (RSE) and the $R^2$ statistic.

-   The RSE gives an absolute \alert{measure of lack of fit of the model to the data.}
-   The $\mathbf{R^2}$ **statistic** measures \alert{the proportion of variability in $Y$ that can be explained by $X$.}

We've already seen how the RSE is computed from the RSS and the $R^2$ statistic can be computed using $$
R^{2}=1-\frac{\mathrm{RSS}}{\mathrm{TSS}}
$$ where $\mathrm{TSS}=\sum\left(y_{i}-\bar{y}\right)^{2}$ is the **total sum of squares** which measures the amount of variability in the responses before regression is performed.

------------------------------------------------------------------------

# Simple Linear Regression Summary

Simple linear regression uses a single predictor variable $X$ to predict a response $Y$ with $$
Y \approx \beta_{0}+\beta_{1} X
$$ 

- $\beta_{0}, \beta_{1}$ are estimated by minimizing the residual sum of squares (RSS)

- The standard error (SE) of the coefficient estimates is a measure of accuracy. 

- The residual standard error (RSE) gives a measure of lack of fit of the model to the data. 

- The $R^2$ statistic measures the proportion of variability explained by the regression. - A hypothesis test on $\beta_1$ indicates whether there is a relationship between $X$ and $Y$. 

- The `lm()` function in R can be used to perform all of these tasks!

**Any Questions?**

------------------------------------------------------------------------

# Exercises: Simple Linear Regression

Open the Linear Regression Exercises R Markdown file.

-   Go over the "Simple Linear Regression" section together as a class.

------------------------------------------------------------------------

# Multiple Linear Regression

Suppose we have $n$ observations in our data each consisting of $p$ predictor values and one response value. That is, $$
\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{n}, y_{n}\right)\right\} \text { where } x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right).
$$ We want to fit this data with a linear model. We can extend simple linear regression to accommodate $p$ predictors. $$
Y=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}+\epsilon
$$ \alert{We interpret $\beta_j$ as the average effect on $Y$ of one unit increase in $X_j$ while holding all other predictors fixed.}

As with simple linear regression, the coefficients $\beta_0, \dots, \beta_p$ are unknown and must be estimated.

------------------------------------------------------------------------

# Estimating the Coefficients

We want to find estimates $\hat \beta_0, \dots, \hat \beta_p$, so that predictions for the response can be made using $$
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}+\cdots+\hat{\beta}_{p} x_{p}.
$$ The least squares approach is used again in this case to estimate the $p$ parameters. That is, we choose $\beta_0, \dots, \beta_p$ to minimize the sum of the squared residuals $$
\begin{aligned}
\mathrm{RSS} &=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2} \\
&=\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i 1}-\hat{\beta}_{2} x_{i 2}-\cdots-\hat{\beta}_{p} x_{i p}\right)^{2}
\end{aligned}
$$ The equations for $\hat \beta_0, \dots, \hat \beta_p$ which minimize the $RSS$ are complicated and not entirely important since there are functions that perform the computation for us in R and Python.

------------------------------------------------------------------------

# Least Squares Regression Plane

::: columns
::: {.column width="50%"}
The figure shows the relationship between two predictor variables and a response variable. Linear regression in this case gives a plane fit by minimizing the squared vertical distance between the observations and the plane.
:::

::: {.column width="50%"}
![](images/regression_plane.png){width="249"}
:::
:::

------------------------------------------------------------------------

# Important Questions

When working with multiple linear regression, we are often interested in several important questions. 

- Is there a relationship between the response and the predictors?

- How well does the model fit the data?

- Given a set of predictor values, what is the predicted response, and how accurate is our prediction? We will go over the methods for answering each of these questions.

------------------------------------------------------------------------

# Hypothesis Test for Parameters

One: *Is there a relationship between the response and the predictors?*

We can address this question by testing whether the regression coefficients are far enough from zero.

Our null hypothesis and alternative hypothesis are the following: $$
H_{0}: \beta_{1}=\beta_{2}=\cdots=\beta_{p}=0
\hspace{1cm}
H_{a}: \beta_{j} \neq 0 \text{ for some } j
$$ The hypothesis can be tested with the $\mathbf{F}$**-statistic**, which is defined by $$
F=\frac{(\mathrm{TSS}-\mathrm{RSS}) / p}{\mathrm{RSS} /(n-p-1)}.
$$ If the $F$-statistic is much larger than 1 we reject the null hypothesis and conclude there is a relationship between at least on the the predictors and the response. If the $F$-statistic is close to 1, the $p$-value can be computed to determine the outcome.

------------------------------------------------------------------------

# $RSE$ and $R^2$

Two: *How well does the model fit the data?*

The $\operatorname{RSE}$ and $R^2$ are measures of the model fit. In the multiple linear regression context, $R^2$ is the square of the correlation between the response and the fitted linear model. That is, $R^2 = \operatorname{Cor}(Y, \hat Y)^2$.

The $\operatorname{RSE}$ is defined by 
$$
\mathrm{RSE}=\sqrt{\frac{\mathrm{RSS}}{n-p-1} } \hspace{1cm} \text{where } n = \text{\# observations},\ p = \text{\# predictors}
$$

Important considerations as the number of variables in the model increases:

-   $R^2$ will increase even if the new variables have a weak association with the response.

-   $RSS$ of the training data will decrease, but not necessarily that of the testing data.

-   $RSE$ will increase if the decrease in $RSS$ is small relative to the increase in $p$.

------------------------------------------------------------------------

# Prediction Accuracy

Three: *Given a set of predictor values, what is the predicted response, and how accurate is our prediction?* Once we have fit the multiple regression model, the response $Y$ is predicted by $$
        \hat Y = \hat{\beta}_{0}+\hat{\beta}_{1} X_{1}+\cdots+\hat{\beta}_{p} X_{p}.
        $$ There are three types of uncertainty associated with the prediction $\hat Y$

1.  The **reducible error** arising from the inaccuracy of the coefficient estimates.

2.  The reducible error stemming from the assumption that the relationship between $Y$ and $X$ is linear; **model bias**.

3.  The **irreducible error** from the random error associated with the true response $Y = f(x) + \epsilon$.

We can address how much $Y$ will vary from $\hat Y$ using prediction intervals.

------------------------------------------------------------------------

# Exercises: Multiple Linear Regression

Open the Linear Regression Exercises R Markdown file.

-   Go over the "Multiple Linear Regression" section together as a class.
