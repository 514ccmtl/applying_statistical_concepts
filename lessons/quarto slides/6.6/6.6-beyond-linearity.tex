% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
  aspectratio=169,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usetheme[]{Madrid}
\usecolortheme{DarkBlue}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\definecolor{DarkBlue}{rgb}{0.05, 0.15, 0.3}
\setbeamercolor{structure}{fg=DarkBlue}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={6.6: Beyond Linearity},
  pdfauthor={Navona Calarco},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{6.6: Beyond Linearity}
\author{Navona Calarco}
\date{}
\institute{The University of Toronto}

\begin{document}
\frame{\titlepage}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, breakable, sharp corners, frame hidden, enhanced, interior hidden]}{\end{tcolorbox}}\fi

\begin{frame}{Introduction}
\protect\hypertarget{introduction}{}
In section 6.5 we saw how we can improve upon linear models by reducing
their complexity, and therefore the resulting variance of the estimates.

In this section we will look at extension of the linear model that do
not require linearity.

\begin{itemize}
\item
  Polynomial Regression
\item
  Step Functions
\item
  Regression Splines
\item
  Local Regression
\item
  Generalized Additive Models
\end{itemize}
\end{frame}

\begin{frame}{Polynomial Regression}
\protect\hypertarget{polynomial-regression}{}
The extension from linear regression to polynomial regression replaces
the linear model \[
y_{i}=\beta_{0}+\beta_{1} x_{i}+\epsilon_{i}
\] with a polynomial model of degree \(d\) \[
y_{i}=\beta_{0}+\beta_{1} x_{i}+\beta_{2} x_{i}^{2}+\beta_{3} x_{i}^{3}+\cdots+\beta_{d} x_{i}^{d}+\epsilon_{i}.
\] The coefficient are easily
\alert{estimated using least squares regression} with predictors
\(x_{i}, x_{i}^{2}, \ldots, x_{i}^{d}\).

\begin{itemize}
\tightlist
\item
  We do not often fit polynomials with degrees higher than 4 since they
  can be overly flexible and result in overfitting.
\end{itemize}
\end{frame}

\begin{frame}{Exercises: Polynomial Regression\}}
\protect\hypertarget{exercises-polynomial-regression}{}
Open the Beyond Linearity Exercises R Markdown file.

\begin{itemize}
\item
  Go over the ``Getting Started'' section together as a class.
\item
  Go over the ``Polynomial Regression'' section together as a class.
\item
  5 minutes for students to complete the questions.
\item
  Questions should be completed at home if time does not allow.
\end{itemize}
\end{frame}

\begin{frame}{Step Functions}
\protect\hypertarget{step-functions}{}
Step functions involve breaking the range of the predictor \(X\) into
sections with specified cut points \(c_1, c_2, \dots c_K\), called
\textbf{knots}. Each section is then fit with a constant so our model
becomes \[
\begin{aligned}
y_i = f(x_i) =\left\{\begin{array}{cl}
\beta_0 + \beta_1, & \text{if} \quad x_i \leq c_1 \\
\beta_0 + \beta_2, & \text{if} \quad c_1 \leq x_i \leq c_2 \\
\vdots \\
\beta_0 + \beta_K, & \text{if} \quad c_{K} \leq x_i\\
\end{array}\right.
\end{aligned}
\] We can use least squares to fit the coefficients.
\end{frame}

\begin{frame}{Exercises: Step Functions}
\protect\hypertarget{exercises-step-functions}{}
Open the Beyond Linearity Exercises R Markdown file.

\begin{itemize}
\item
  Go over the ``Step Functions'' section together as a class.
\item
  5 minutes for students to complete the questions.
\end{itemize}

-Questions should be completed at home if time does not allow.
\end{frame}

\begin{frame}{Regression Splines}
\protect\hypertarget{regression-splines}{}
\textbf{Piecewise polynomial regression} is a combination of the
previous two methods we have seen. We fit each of the \(K\) predictor
sections with a polynomial of degree \(d\). \[
\begin{aligned}
y_i = f(x_i) =\left\{\begin{array}{cl}
\beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \cdots + \beta_{d1}x_i^d, & \text{if} \quad x_i \leq c_1 \\
\beta_{02} + \beta_{12}x_i + \beta_{22}x_i^2 + \cdots + \beta_{d2}x_i^d, & \text{if} \quad c_1 \leq x_i \leq c_2 \\
\vdots \\
\beta_{0K} + \beta_{1K}x_i + \beta_{2K}x_i^2 + \cdots + \beta_{dK}x_i^d, & \text{if} \quad c_K \leq x_i \\
\end{array}\right.
\end{aligned}
\] Again, the coefficients can be fit using least squares.

The \textbf{degrees of freedom} of a model is a measure of how flexible
it is.

\begin{itemize}
\item
  The higher the degree of the polynomial and the more knots we have,
  the higher the degree of freedom.
\item
  The higher the degrees of freedom the more flexible the model is.
\end{itemize}
\end{frame}

\begin{frame}{Location and Number of Knot}
\protect\hypertarget{location-and-number-of-knot}{}
Where to place knots?

\begin{itemize}
\item
  Regression splines are most stable in regions with a lot of knots.
\item
  Place knots in places where function might vary the fastest.
\item
  Although often knots are placed at uniform intervals for simplicity.
\end{itemize}

How many knots?

\begin{itemize}
\item
  Try out different numbers and see which looks best.
\item
  Use cross-validation to find the value of \(K\) with the smallest RSS.
\end{itemize}
\end{frame}

\begin{frame}{Constraints for Regression Splines}
\protect\hypertarget{constraints-for-regression-splines}{}
There are several useful constraints that can be used with regression
splines to reduce their degrees of freedom, thereby reducing their
flexibility and making the fit more stable.

\begin{itemize}
\item
  \alert{Continuity}: the fitted curve must be continuous.
\item
  \alert{Continuous derivatives}: the first or second derivative must be
  continuous (this has the effect of requiring the curve to be
  smooth\ldots{} no corners etc.)
\item
  \alert{Natural spline}: the function must be linear in the region on
  the boundary (i.e.~where \(X\) is smaller than the smallest knot or
  larger than the largest knot).
\end{itemize}
\end{frame}

\begin{frame}{Exercises: Splines}
\protect\hypertarget{exercises-splines}{}
Open the Beyond Linearity Exercises R Markdown file.

\begin{itemize}
\tightlist
\item
  Go over the ``Splines'' section together as a class.
\end{itemize}
\end{frame}

\begin{frame}{Local Regression}
\protect\hypertarget{local-regression}{}
Local regression involves fitting a function at a target point \(x_0\)
using only the nearby training observations.

\begin{enumerate}
\item
  Find the \(k\) training observations with \(x_i\) closest to \(x_0\).
\item
  Assign weights \(K_{i0}\) to each of these points so that the farthest
  point has weight zero and the closest has the highest weight.
\item
  Fit a weighted least squares regression by finding \(\hat{\beta_0}\)
  and \(\hat{\beta_1}\) that minimize
  \[\sum_{i=1}^{n} K_{i 0}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2}.\]
\item
  The fitted function is then
  \(y_i = \hat{\beta_0} + \hat{\beta_1} x_i\).
\end{enumerate}
\end{frame}

\begin{frame}{Choices for Local Regression}
\protect\hypertarget{choices-for-local-regression}{}
The most important choice is the
\alert{span $s = k/n$ which is the fraction of points that are used to perform the local regression}.

\begin{itemize}
\item
  Controls the flexibility of the fit

  \begin{itemize}
  \item
    small \(s\) results in a local, more wiggly fit
  \item
    large \(s\) provides a global fit.
  \end{itemize}
\item
  Cross-validation can be used to choose \(s\) or it can be chosen
  directly.
\end{itemize}

There are other less important choices that can be made.

\begin{itemize}
\item
  How to define the weighting function \(K\).
\item
  What type of model to use such as constant, linear, or quadratic
  regression.
\end{itemize}
\end{frame}

\begin{frame}{Exercises: Local Regression}
\protect\hypertarget{exercises-local-regression}{}
Open the Beyond Linearity Exercises R Markdown file.

\begin{itemize}
\item
  Go over the ``Local Regression'' section together as a class.
\item
  10 minutes for students to complete the questions.
\item
  Questions should be completed at home if time does not allow.
\end{itemize}
\end{frame}

\begin{frame}{Generalized Additive Models}
\protect\hypertarget{generalized-additive-models}{}
We have seen how we can extend simple linear regression beyond
linearity. Now we will look at extensions for multiple linear
regression. In the case of a \alert{quantitative response} our linear
regression model is
\[y_{i}=\beta_{0}+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\cdots+\beta_{p} x_{i p}+\epsilon_{i}\]
We can replace the linear \(\beta_{j} x_{i j}\) with a non linear
function \(f_{j}\left(x_{i j}\right)\) to aquire the model \[
\begin{aligned}
y_{i} =\beta_{0}+f_{1}\left(x_{i 1}\right)+f_{2}\left(x_{i 2}\right)+\cdots+f_{p}\left(x_{i p}\right)+\epsilon_{i}
\end{aligned}
\] The functions \(f_{j}\left(x_{i j}\right)\) can all have different
forms including natural splines, polynomial regression, local regression
etc. We can fit the model using least squares.
\end{frame}

\begin{frame}{Generalized Additive Models}
\protect\hypertarget{generalized-additive-models-1}{}
GAMs can also be used for qualitative \(Y\). We will assume that \(Y\)
is zero or one and \(p(X) = \operatorname{Pr}(Y = 1|X)\). The logistic
regression model is \[
\log \left(\frac{p(X)}{1-p(X)}\right)=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}
\] We can incorporate non-linear relationships with functions
\(f_j(X_j)\) \[
\log \left(\frac{p(X)}{1-p(X)}\right)=\beta_{0}+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\cdots+f_{p}\left(X_{p}\right)
\]
\end{frame}

\begin{frame}{Pros and Cons of GAMs}
\protect\hypertarget{pros-and-cons-of-gams}{}
Pros

\begin{itemize}
\item
  We can model different non-linear relationships for each variable.
\item
  Non-linear fits could provide more accurate predictions.
\item
  The additive model allows for the examination of the effect of a
  single predictor while holding the rest constant.
\end{itemize}

Cons

\begin{itemize}
\tightlist
\item
  The interactions between variables are missed with the additive
  assumption. However, we can add additional predictors in the form of
  \(X_i \times X_j\).
\end{itemize}
\end{frame}

\begin{frame}{Exercises: GAMs}
\protect\hypertarget{exercises-gams}{}
Open the Beyond Linearity Exercises R Markdown file.

\begin{itemize}
\item
  Go over the start of the ``Generalised Linear Models'' section
  together as a class.
\item
  10 minutes for students to complete the questions.
\item
  Go over the rest of the ``Generalised Linear Models'' section together
  as a class.
\item
  Questions should be completed at home if time does not allow.
\end{itemize}
\end{frame}

\begin{frame}{References}
\protect\hypertarget{references}{}
Chapter 7 of the ISLR2 book:

James, Gareth, et al.~``Moving Beyond Linearity.'' An Introduction to
Statistical Learning: with Applications in R, 2nd ed., Springer, 2021.
\end{frame}



\end{document}
