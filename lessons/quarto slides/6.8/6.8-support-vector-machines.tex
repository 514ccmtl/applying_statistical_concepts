% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
  aspectratio=169,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usetheme[]{Madrid}
\usecolortheme{DarkBlue}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\definecolor{DarkBlue}{rgb}{0.05, 0.15, 0.3}
\setbeamercolor{structure}{fg=DarkBlue}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={6.8: Support Vector Machines},
  pdfauthor={Simone Collier},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{6.8: Support Vector Machines}
\author{Simone Collier}
\date{}
\institute{The University of Toronto}

\begin{document}
\frame{\titlepage}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, frame hidden, interior hidden, breakable, enhanced, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt]}{\end{tcolorbox}}\fi

\begin{frame}{Introduction}
\protect\hypertarget{introduction}{}
The support vector machine (SVM) is an approach for classification in a
binary setting. We will cover:

\begin{itemize}
\item
  The maximal margin classifier
\item
  The support vector classifier
\item
  The support vector machine
\end{itemize}

The first two methods are specific cases of the support vector machine,
but they can be very useful given the right scenario. We will start by
introducing the concept of a hyperplane which is what these methods all
rely on.
\end{frame}

\begin{frame}{Hyperplane}
\protect\hypertarget{hyperplane}{}
In a \(p\) dimensional space, a hyperplane is a flat \(p-1\) dimensional
subspace.

\begin{itemize}
\item
  In two dimensions, a hyperplane is a flat one dimensional subsapce (a
  line) defined by \[\beta_{0}+\beta_{1} X_{1} = 0\] for parameters
  \(\beta_{0}\), and \(\beta_{1}\). That is, any \(X = (X_1, X_2)\) that
  satisfies that equation is on the hyperplane.
\item
  In three dimensions a hyperplane is a flat two dimensional subspace (a
  plane) defined by
  \[\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}=0\]
\item
  In \(p\) dimensions it is a \(p-1\) dimensional flat subspace defined
  by
  \[\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}=0\]
\end{itemize}
\end{frame}

\begin{frame}{Hyperplane}
\protect\hypertarget{hyperplane-1}{}
A hyperplane divides a \(p\) dimensional space into two.

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{figure}

{\centering \includegraphics[width=2.83333in,height=\textheight]{images/hyperplane.png}

}

\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{itemize}
\item
  \(\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}>0\)
  implies that \(X\) is not on the hyperplane and is instead on one side
  of it.
\item
  \(\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}<0\)
  implies that \(X\) on the other side of the hyperplane.
\item
  The hyperplane in the figure is \(1 + 2X_1 + 3X_2 = 0\)
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Classification Using a Separating Hyperplane}
\protect\hypertarget{classification-using-a-separating-hyperplane}{}
We have the following data:

\begin{itemize}
\item
  \(n\) training observations \(x_1, \dots, x_n\), each of which are
  \(p\) dimensional vectors \(x_i = (x_{i1}, \dots, x_{ip})\)
\item
  Qualitative response \(y_1, \dots, y_n \in \{-1, 1\}\) where -1
  represents one class and 1 represents the other.
\item
  Test observation \(x^* = (x^*_1, \dots, x^*_p)\)
\end{itemize}

The goal is to
\alert{develop a classifier from the training data that will correctly classify the test observation}.
\end{frame}

\begin{frame}{Classification Using a Separating Hyperplane}
\protect\hypertarget{classification-using-a-separating-hyperplane-1}{}
Suppose that is it possible to construct a hyperplane that separates the
training observations according to their class. An example in 2
dimensions:

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{figure}

{\centering \includegraphics[width=2.60417in,height=\textheight]{images/sep_hyperplane.png}

}

\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{itemize}
\item
  \(\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}>0\) implies that \(X\) is
  part of the blue class.
\item
  \(\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}>0\) implies that \(X\) is
  part of the purple class.
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Classification Using a Separating Hyperplane}
\protect\hypertarget{classification-using-a-separating-hyperplane-2}{}
In our problem, we are trying to separate the \(y_i = 1\) class from the
\(y_i = -1\) class which could also just be thought of as the blue class
and the purple class.

Our separating hyperplane is constructed based on the properties:

\begin{itemize}
\item
  \(\beta_{0}+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\cdots+\beta_{p} x_{i p}>0\)
  if \(y_{i}=1\)
\item
  \(\beta_{0}+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\cdots+\beta_{p} x_{i p}<0\)
  if \(y_{i}=-1\)
\end{itemize}

If a separating hyperplane exists then we can use it to
\alert{classify test observations by what side of the hyperplane they are on}.

\begin{itemize}
\item
  Classify \(x^*\) based on the sign of
  \(f(x^*) = \beta_{0}+\beta_{1} x_{1}^{*}+\beta_{2} x_{2}^{*}+\cdots+\beta_{p} x_{p}^{*}\).
\item
  If \(f(x^*)\) is far from zero then \(x^*\) is far from the hyperplane
  and we can be more confident in our classification than if \(x^*\)
  were close to the hyperplane.
\end{itemize}
\end{frame}

\begin{frame}{The Maximal Margin Classifier}
\protect\hypertarget{the-maximal-margin-classifier}{}
If our data can be separated by hyperplane then there will be infinitely
many separating hyperplanes. The
\alert{maximal margin hyperplane is the separating hyperplane that is farthest from the training observations}.

\begin{itemize}
\item
  Compute the perpendicular distance from each training observation to a
  given separating hyperplane (known as the \textbf{margin=}).
\item
  The maximal margin hyperplane is the hyperplane that maximizes the
  margin.
\item
  Classify a test observation based on which side of the maximal margin
  hyperplane it is on.
\end{itemize}
\end{frame}

\begin{frame}{The Maximal Margin Classifier}
\protect\hypertarget{the-maximal-margin-classifier-1}{}
\begin{columns}[T]
\begin{column}{0.45\textwidth}
\begin{figure}

{\centering \includegraphics[width=2.60417in,height=\textheight]{images/maxmargin.png}

}

\end{figure}
\end{column}

\begin{column}{0.55\textwidth}
\begin{itemize}
\item
  The maximal margin classifier is the solid line.
\item
  The margin is the distance from the solid line to the dashed lines.
\item
  The three observations that are on the dashed lines are equidistant
  from the hyperplane and are called \textbf{support vectors}.
\item
  The maximal margin classifier only depends directly on the support
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{The Maximal Margin Classifier}
\protect\hypertarget{the-maximal-margin-classifier-2}{}
A separating hyperplane classifier will necessarily perfectly classify
the training observations. This can lead to
\alert{sensitivity to some observations and overfitting}.

\begin{columns}[T]
\begin{column}{0.6\textwidth}
\begin{figure}

{\centering \includegraphics{images/margin_sensitivity.png}

}

\end{figure}
\end{column}

\begin{column}{0.4\textwidth}
\begin{itemize}
\item
  Left: Maximal margin hyperplane separates two classes.
\item
  Right: An additional blue training observation is added to the
  training set causing the hyperplane to shift dramatically.
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Exercises: The Maximal Margin Classifier}
\protect\hypertarget{exercises-the-maximal-margin-classifier}{}
Open the Support Vector Machines Exercises R Markdown file.

\begin{itemize}
\tightlist
\item
  Go over the ``Maximal Margin Classifier'' section together as a class.
\end{itemize}
\end{frame}

\begin{frame}{Support Vector Classifier}
\protect\hypertarget{support-vector-classifier}{}
The support vector classifier is an extension of the maximal margin
classifier that uses a
\alert{\textbf{soft margin} which does not perfectly separate the two classes}.

\begin{itemize}
\item
  Greater robustness to individual observations.
\item
  Better classification of most of the training observations.
\item
  Can accommodate data sets that are not perfectly separable by a
  hyperplane.
\end{itemize}

The idea is that
\alert{misclassifying a few training observations could help to better classify the remaining observations}.
\end{frame}

\begin{frame}{Support Vector Classifier}
\protect\hypertarget{support-vector-classifier-1}{}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{figure}

{\centering \includegraphics[width=2.60417in,height=\textheight]{images/soft_margin.png}

}

\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{itemize}
\item
  The support vector classifier hyperplane is the solid line and the
  margins are the dashed lines.
\item
  The observations below the hyperplane are classified as purple and
  those above are classified as blue.
\item
  Observations 1 and 8 are intentionally on the wrong side of the
  margin.
\item
  Observations 11 and 12 are intentionally on the wrong side of the
  hyperplane and the wrong side of the margin.
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Support Vector Classifier}
\protect\hypertarget{support-vector-classifier-2}{}
The support vector classifier is constructed using several parameters,
the most important being the tuning parameter \(C\) which
\alert{determines the number of and severity of violations to the margin and hyperplane}
that we will allow.

\begin{itemize}
\item
  No more than \(C\) observations can be on the wrong side of the
  hyperplane.
\item
  \(C = 0\) implies that no violations will be allowed so this gives the
  maximal margin hyperplane.
\item
  As \(C\) increases the margin will widen.
\item
  \(C\) is chosen using cross-validation.
\item
  \(C\) controls the bias-variance trade-off of the model.

  \begin{itemize}
  \tightlist
  \item
    if \(C\) is small, then the classifier is highly fit to the data
    which yields low bias, high variance.
  \item
    if \(C\) is larger, then the classifier is fit less hard to the data
    which yields reduces variance and increases bias.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Support Vector Classifier}
\protect\hypertarget{support-vector-classifier-3}{}
\begin{itemize}
\item
  The support vector classifier
  \alert{aims to make $M$, the width of the margin, as large as possible while staying within the budget of margin violations $C$}.
\item
  Observations that lie directly on the margin or on the wrong side of
  it are \textbf{support vectors}.
\item
  The support vector classifier is
  \alert{only depends directly on the support vectors}.
\item
  As \(C\) increases, so does the number of support vectors. This means
  there are more observations that determine the hyperplane (hence lower
  variance).
\end{itemize}
\end{frame}

\begin{frame}{Support Vector Classifier}
\protect\hypertarget{support-vector-classifier-4}{}
\begin{columns}[T]
\begin{column}{0.6\textwidth}
\begin{figure}

{\centering \includegraphics[width=2.91667in,height=\textheight]{images/4SVC.png}

}

\end{figure}
\end{column}

\begin{column}{0.4\textwidth}
A support vector classifier fit to the same data set with four different
different values of \(C\).
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Exercises: Support Vector Classifier}
\protect\hypertarget{exercises-support-vector-classifier}{}
Open the Support Vector Machines Exercises R Markdown file.

\begin{itemize}
\item
  Go through the ``Support Vector Classifier'' section together as a
  class.
\item
  When a question is reached, allow 10 minutes for the students to work
  on it.
\item
  Questions should be completed at home if time does not allow.
\item
  Go over the rest of the ``Support Vector Classifier'' section together
  as a class.
\end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines}
\protect\hypertarget{support-vector-machines}{}
The support vector machine (SVM) is an extension of the support vector
classifier that
\alert{enlarges the feature space in order to accommodate a non-linear boundary}
between classes.

\begin{itemize}
\item
  Uses \textbf{kernals} which are functions that quantify similarities
  between two observations.
\item
  The support vector classifier happens to be fit with kernals as well,
  namely a polynomial kernal of degree 1 (linear).
\item
  SVMs use non-linear kernals such as high degree polynomial kernals or
  radial kernals in order to get a more flexible boundary.
\item
  The technical details of SVMs are out of scope for this course.
\end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines}
\protect\hypertarget{support-vector-machines-1}{}
A data set with two classes is fit with a support vector classifier. The
linear boundary does not perform well.

\begin{figure}

{\centering \includegraphics[width=4.16667in,height=\textheight]{images/SVCpoor.png}

}

\end{figure}
\end{frame}

\begin{frame}{Support Vector Machines}
\protect\hypertarget{support-vector-machines-2}{}
The data set is now fit with two different support vector machines.

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{figure}

{\centering \includegraphics[width=3.125in,height=\textheight]{images/SVMperformance.png}

}

\end{figure}
\end{column}

\begin{column}{0.3\textwidth}
\begin{itemize}
\item
  Left: SVM with a polynomial kernal of degree 3.
\item
  Right: SVM with a radial kernal.
\end{itemize}
\end{column}
\end{columns}

These SVMs capture the decision boundary much better than the support
vector classifier.
\end{frame}

\begin{frame}{SVMs with More than Two Classes}
\protect\hypertarget{svms-with-more-than-two-classes}{}
Extending the concept of separating hyperplanes to \(K > 2\) classes is
actually quite tricky. The two main approaches for this are briefly
described.

\begin{itemize}
\item
  One-versus-one classification

  \begin{enumerate}
  \item
    We construct SVMs to compare each combination of two classes.
  \item
    We classify a test observation to one of two classes using each of
    the SVM classifiers.
  \item
    The test observation is finally assigned to the class to which is
    was most frequently classified by the SVMs.
  \end{enumerate}
\item
  One-versus-all classification

  \begin{enumerate}
  \item
    We construct \(K\) SVMs which each compare one of the classes to the
    rest of the \(K-1\) classes.
  \item
    We assign the observation to the class for which the observation is
    the farthest away from the hyperplane (on the correct side).
  \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Exercises: Support Vector Machine}
\protect\hypertarget{exercises-support-vector-machine}{}
Open the Support Vector Machines Exercises R Markdown file.

\begin{itemize}
\item
  Go through the ``Support Vector Machine'' section together as a class.
\item
  10 minutes to complete the questions at the end of the section.
\item
  Questions should be completed at home if time does not allow.
\end{itemize}
\end{frame}

\begin{frame}{References}
\protect\hypertarget{references}{}
Chapter 9 of the ISLR2 book:

James, Gareth, et al.~``Support Vector Machines.'' An Introduction to
Statistical Learning: with Applications in R, 2nd ed., Springer, 2021.
\end{frame}



\end{document}
