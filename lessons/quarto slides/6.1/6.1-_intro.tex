% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
  aspectratio=169,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usetheme[]{Madrid}
\usecolortheme{DarkBlue}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\definecolor{DarkBlue}{rgb}{0.05, 0.15, 0.3}
\setbeamercolor{structure}{fg=DarkBlue}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={6.1: Introduction to Statistical Learning},
  pdfauthor={Navona Calarco},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{6.1: Introduction to Statistical Learning}
\author{Navona Calarco}
\date{}
\institute{The University of Toronto}

\begin{document}
\frame{\titlepage}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, breakable, enhanced, interior hidden, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
\begin{frame}[allowframebreaks]
  \frametitle{Table of contents}
  \tableofcontents[hideallsubsections]
\end{frame}
\begin{frame}{What is Statistical Learning?}
\protect\hypertarget{what-is-statistical-learning}{}
Suppose we want to figure out the
\alert{association between the allocation of advertising budgets and sales}
in order to increase sales for a client.

\begin{itemize}
\item
  There are three types of advertising: TV, radio, and newspaper which
  can be labelled \(X_1, X_2, and\ X_3\) respectively.
\item
  The advertising budgets are the independent variables, or
  \textbf{predictor variables}.
\item
  The sales is the dependent variable, or \textbf{response variable}
  which we label Y.When you click the \textbf{Render} button a document
  will be generated that includes:
\item
  Each observation in the dataset has a value for the predictors
  \(X_1, X_2, X_3,\) and the response \(Y\).
\end{itemize}
\end{frame}

\begin{frame}{What is Statistical Learning?}
\protect\hypertarget{what-is-statistical-learning-1}{}
The sales in relation to each of the advertising budgets are shown along
with a simple fitted line for the relationships.

\begin{figure}

{\centering \includegraphics[width=5.73958in,height=\textheight]{images/sales fig.png}

}

\end{figure}
\end{frame}

\begin{frame}{What is Statistical Learning?}
\protect\hypertarget{what-is-statistical-learning-2}{}
We want to find the relationship between the predictor variables and the
response variable.

We can assume that there is a relationship between
\(X = (X_1, X_2, X_3)\) and \(Y\) which can be written in the general
form

\begin{block}{}
$$Y = f(X) + \epsilon$$

\begin{itemize}
    \item $f$ is a fixed unknown function of the predictor variables.
    \item $\epsilon$ is a random error term which has mean zero.
\end{itemize}
\end{block}

\alert{Statistical learning is summarized by the set of approached which are used to estimate $f$.}
\end{frame}

\begin{frame}{Prediction vs Inference}
\protect\hypertarget{prediction-vs-inference}{}
The are two main reasons for why we want to estimate \(f\):

\begin{enumerate}
\item
  \textbf{Prediction} We want to know
  \alert{what response is expected given a set of predictors}. Ex: What
  income is expected for a given level of education and seniority?
\item
  \textbf{Inference} We want to understand
  \alert{how the response variable is affected by changes in the predictors}.
  Ex: To what extent is income associated with education?
\end{enumerate}
\end{frame}

\begin{frame}{Prediction}
\protect\hypertarget{prediction}{}
Prediction problems often arise when the
\alert{predictor variables $X$ are known but the response $Y$ is not easily obtained}.
Recall the general form for the relationship between the predictor and
response variables

\[Y = f(X) + \epsilon\]

Since the error term \(\epsilon\) averages to zero, we can go about
predicting \(Y\) using

\[\hat Y = \hat f(X)\]

We use '' \$\textbackslash hat \textbackslash{} \$ '' to denote
estimates. That is, \(\hat Y\) is an estimate for \(Y\) and \(\hat f\)
is an estimate for \(f\).
\end{frame}

\begin{frame}{Prediction}
\protect\hypertarget{prediction-1}{}
The accuracy of our prediction \(\hat Y\) depends on:

\begin{enumerate}
\tightlist
\item
  The \textbf{reducible error}: the error in our estimate \(\hat f\).
  This error is reducible since estimates can always be improved.
\item
  The \textbf{irreducible error}: the random error associated with the
  true response \(Y = f(x) + \epsilon\). (Even if \(\hat f = f\),
  \(\hat Y\) will still have error associated with its prediction since
  \(\epsilon\) is not a function of \(X\).)
\end{enumerate}
\end{frame}

\begin{frame}{Inference}
\protect\hypertarget{inference}{}
We want to know how the predictor variables and the response variable
are related.

\begin{enumerate}
\tightlist
\item
  \emph{Which predictors affect the response?}
\item
  \emph{Is a linear equation a good approximation for the relationship
  between the predictors and the response?}
\end{enumerate}

In each case, we do not want to make predictions for \(Y\) using
\(\hat f\), we want to find the true form of \(f\).
\end{frame}

\begin{frame}{How do we Estimate \(f\)?}
\protect\hypertarget{how-do-we-estimate-f}{}
Assume that we have \(n\) observations in our data set. The standard
approach is to split the data set into training data and testing data.

\begin{itemize}
\tightlist
\item
  \textbf{training data} is used to train or teach the statistical
  method we are using to estimate \(f\).
\item
  \textbf{testing data} is used to test the accuracy of the resulting
  estimate for \(f\) on new data.
\end{itemize}

In general, the statistical learning methods used to estimate \(f\) can
be characterized as \textbf{parametric}, and \textbf{non-parametric}.
\end{frame}

\begin{frame}{Parametric Methods}
\protect\hypertarget{parametric-methods}{}
This approach is implemented in two steps:

\begin{itemize}
\tightlist
\item
  Make an assumption about the functional form of \(f\).
\item
  Develop a procedure to fit the model to the training data.
\end{itemize}

\begin{block}{Example}
\protect\hypertarget{example}{}
\begin{itemize}
\tightlist
\item
  Suppose \(f\) is linear in \(X\):
  \(f(X)=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}\).
  This assumption has reduced the number of parameters that need to be
  fit significantly compared to fitting a generic p-dimensional
  function.
\item
  We need to estimate
  \(\beta_{0}, \beta_{1}, \beta_{2}, \dots, \beta_{p}\) such that
  \(Y \approx \beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}\).
  One approach is to use least squares which attempts to minimize the
  difference between the data and our estimate for \(f\).
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Non-Parametric Methods}
\protect\hypertarget{non-parametric-methods}{}
This method does not make an assumption about the form of \(f\).
Instead, the goal of non-parametric fitting it to
\alert{create an estimate for $f$ that follows the data as close as possible while staying as "smooth" as possible}.

\begin{itemize}
\tightlist
\item
  This avoids the danger seen in parametric approaches where the
  functional form of the estimate could be completely different from the
  true \(f\).
\item
  This approach requires many parameters to be fit since the from of
  \(f\) needs to be flexible.
\item
  As a result, many observations are needed in order to get an accurate
  estimate for \(f\).
\item
  This approach could lead to \textbf{overfitting}, in which \(f\)
  follows the noise and random variation in our data too closely.
\item
  Splines are an example of non-parametric fitting.
\end{itemize}
\end{frame}

\begin{frame}{Accuracy-Interpretability Trade-Off}
\protect\hypertarget{accuracy-interpretability-trade-off}{}
\begin{itemize}
\tightlist
\item
  The methods we will introduce have different levels of restrictiveness
  or flexibility.
\item
  Choosing a model on the basis of flexibility will depend on the
  problem at hand.

  \begin{itemize}
  \tightlist
  \item
    \alert{If we are interested in inference, restrictive models are much more interpretable}
    (i.e.~the relationship between the predictors and the response is
    more clear)
  \item
    \alert{If we are only interested in prediction accuracy, flexible models \textbf{might} perform better.}
  \end{itemize}
\item
  Flexible models will not always provide better predictions since they
  are very prone to overfitting!
\end{itemize}
\end{frame}

\begin{frame}{Supervised vs Unsupervised Learning}
\protect\hypertarget{supervised-vs-unsupervised-learning}{}
\begin{itemize}
\tightlist
\item
  \textbf{Supervised learning} involves models for predicting a response
  based on predictor variables

  \begin{itemize}
  \tightlist
  \item
    Examples: linear regression, boosting, support vector machines (SVM)
  \end{itemize}
\item
  \textbf{Unsupervised learning} refers to models used to investigate
  features associated with observations.

  \begin{itemize}
  \tightlist
  \item
    There is no response variable to predict.
  \item
    The goal is to understand the relationship between variables or
    observations.
  \item
    Example: clustering
  \end{itemize}
\item
  \textbf{Semi-supervised learning} involves a set of observations, some
  of which have both predictor and response variables and some with only
  predictor variables. (Not covered in this module)
\end{itemize}
\end{frame}

\begin{frame}{Regression vs Classification Problems}
\protect\hypertarget{regression-vs-classification-problems}{}
\begin{itemize}
\tightlist
\item
  Variables can be either qualitative or quantitative.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Quantitative} variables have numerical values (ex: age,
    monetary value, etc.)
  \item
    \textbf{Qualitative} variables are categorical values (ex: \{small,
    medium, large\} or \{yes, no\})
  \end{itemize}
\item
  \alert{Problems that involve quantitative response variables are \textbf{regression} problems.}
\item
  \alert{Problems that involve qualitative response variables are \textbf{lassification} problems.}
\item
  This is a bit of a generalisation (logistic regression is a
  classification method but its output is numerical so can be thought of
  as a regression method as well)
\end{itemize}
\end{frame}

\begin{frame}{Assessing Model Accuracy}
\protect\hypertarget{assessing-model-accuracy}{}
There is no method that works the best on all data sets so we need a way
to assess the quality of the model's fit to the data.

\begin{itemize}
\tightlist
\item
  Recall that we use the training data set to fit the model.
\item
  Then the test data set is used to see how well the model performs on
  new data by computing some test statistic.
\item
  For regression, the \textbf{mean squared error} (MSE) is most common
  which measures how close the predicted responses are to the true
  responses.
\item
  We can compute the MSE for both the training data and the test data.

  \begin{itemize}
  \tightlist
  \item
    The training MSE will usually be lower (better) than the test MSE.
  \item
    We want to \alert{choose the model that minimizes the test MSE}
    since we only really care about the performance of the model on new
    data.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Assessing Model Accuracy}
\protect\hypertarget{assessing-model-accuracy-1}{}
The left plot shows three different models fit to data. The right plot
shows the training MSE (grey) and test MSE (red) versus the flexibility
of the model.

\begin{columns}[T]
\begin{column}{0.67\textwidth}
\begin{figure}

\includegraphics[width=3.80208in,height=\textheight]{images/test_training_mse.png} \hfill{}

\end{figure}
\end{column}

\begin{column}{0.33\textwidth}
\begin{itemize}
\tightlist
\item
  The least flexible model (orange) has the worst training and test MSE
  so it does not fit the data well.
\item
  The most flexible model (green) has the lowest training MSE but a much
  higher test MSE so the model is overfit.
\item
  The blue model has the lowest test MSE so it is the winner.
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Assessing Model Accuracy}
\protect\hypertarget{assessing-model-accuracy-2}{}
What if no test observations are available?

\begin{itemize}
\tightlist
\item
  As we saw, there is no guarantee that the model that minimises the
  training MSE will minimise the test MSE.
\item
  As model flexibility increases, the training MSE decreases but the
  test MSE may not.
\item
  A model is said to be overfit if a less flexible model would results
  in a lower test MSE.
\item
  An overfit model is picking up on patterns in the training data that
  are not in the test data.
\item
  We will cover many approaches for estimating the test MSE from
  training data.
\end{itemize}
\end{frame}

\begin{frame}{The Bias-Variance Trade-Off}
\protect\hypertarget{the-bias-variance-trade-off}{}
The test MSE will always exhibit a U-shaped curve as a function of model
flexibility. This is because the expected test MSE for some observation
\(x_0\) is the sum of:

\begin{itemize}
\tightlist
\item
  \(\operatorname{Var}\left(\hat{f}\left(x_{0}\right)\right)\): the
  variance of \(\hat{f}\left(x_{0}\right)\) (the variance of the
  predicted response for the test observation \(x_0\) given many
  \(\hat{f}\) fit on different training sets).
\item
  \(\left[\operatorname{Bias}\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}\):
  The squared bias of \(\hat{f}\left(x_{0}\right)\).
\item
  \(\operatorname{Var}(\epsilon)\): the variance of the error terms
  \(\epsilon\).
\end{itemize}
\end{frame}

\begin{frame}{The Bias-Variance Trade-Off}
\protect\hypertarget{the-bias-variance-trade-off-1}{}
\alert{In order to minimise the expected test error we need to use statistical learning methods that result in low bias \textit{and} low variance.}

\begin{itemize}
\tightlist
\item
  \textbf{Variance} is the amount \(\hat{f}\) would change if we fit it
  using a different training set.

  \begin{itemize}
  \tightlist
  \item
    More flexible models have higher variance since they fit the
    training data more closely.
  \end{itemize}
\item
  \textbf{Bias} is the error from approximating a complicated
  relationship with a simpler model.

  \begin{itemize}
  \tightlist
  \item
    More restrictive models have higher bias since they make more
    assumptions about the form of \(f\).
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The Bias-Variance Trade-Off}
\protect\hypertarget{the-bias-variance-trade-off-2}{}
The rate of change of the bias versus variance determines whether the
test MSE will decrease or increase with flexibility.

\begin{itemize}
\tightlist
\item
  Initially, as the flexibility of the method increases, the bias
  decreases faster than the variance increases.

  \begin{itemize}
  \tightlist
  \item
    \(\Rightarrow\) the test MSE declines.
  \end{itemize}
\item
  At some point the increasing flexibility has little impact on the bias
  but the variance increases significantly.

  \begin{itemize}
  \tightlist
  \item
    \(\Rightarrow\) The test MSE increases. -This results in a U-shaped
    curve for test MSE vs method flexibility.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The Bias-Variance Trade-Off}
\protect\hypertarget{the-bias-variance-trade-off-3}{}
A plot of the bias (blue), variance (orange), variance of the error
(dashed line), and the test MSE (red) versus method flexibility for
three different data sets.

\begin{columns}[T]
\begin{column}{0.7\textwidth}
\includegraphics[width=4.23958in,height=\textheight]{images/bias_variance.png}
\end{column}

\begin{column}{0.3\textwidth}
\begin{itemize}
\tightlist
\item
  The MSE is the sum of the other three curves.
\item
  The MSE cannot be smaller than the variance of the error (irreducible
  error).
\item
  The middle model is close to linear so the test MSE immediately
  increases with flexibility.
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Classification Model Accuracy}
\protect\hypertarget{classification-model-accuracy}{}
So far we have discussed model accuracy in the context of regression but
the same idea apply to classification.

\begin{itemize}
\tightlist
\item
  The most common approach for assessing model accuracy is the
  \textbf{training error rate} which is the
  \alert{proportion of misclassified training observations}.
\item
  The \textbf{test error rate} is what we are actually interested in and
  hoping to minimise.
\item
  The bias-variance trade-off is what controls the test error rate in
  the classification context as well.
\end{itemize}
\end{frame}

\begin{frame}{References}
\protect\hypertarget{references}{}
Chapter 2 of the ISLR2 book:

James, Gareth, et al.~``Statistical Learning.'' An Introduction to
Statistical Learning: with Applications in R, 2nd ed., Springer, 2021.
\end{frame}



\end{document}
