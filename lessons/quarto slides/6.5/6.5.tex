% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
  aspectratio=169,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usetheme[]{Madrid}
\usecolortheme{DarkBlue}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\definecolor{DarkBlue}{rgb}{0.05, 0.15, 0.3}
\setbeamercolor{structure}{fg=DarkBlue}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={6.5: Linear Model Selection and Regularization},
  pdfauthor={Navona Calarco},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{6.5: Linear Model Selection and Regularization}
\author{Navona Calarco}
\date{}
\institute{The University of Toronto}

\begin{document}
\frame{\titlepage}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, frame hidden, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, sharp corners, enhanced, interior hidden]}{\end{tcolorbox}}\fi

\begin{frame}{Introduction}
\protect\hypertarget{introduction}{}
In this section we will look at alternative fitting procedures for
linear models that may give better prediction accuracy of model
interpretability compared to the standard least squares approach. The
two classes of methods we will cover in this section are:

\begin{itemize}
\item
  \textbf{Subset Selection}
\item
  \textbf{Shinkage}
\end{itemize}
\end{frame}

\begin{frame}{Subset Selection}
\protect\hypertarget{subset-selection}{}
Subset selection refers to the process of fitting a statistical model
using only a subset of the predictors that are thought to be associated
with the response. This can have the following results:

\begin{itemize}
\item
  Improved \alert{model interpretability} by removing irrelevant
  variables, thereby reducing the complexity of the model.
\item
  Improved \alert{prediction accuracy} by a reduction in the risk of
  overfitting and the variance in the fitted coefficients.
\end{itemize}

We will cover two methods for subset selection:

\begin{itemize}
\item
  Best Subset Selection
\item
  Stepwise Model Selection
\end{itemize}
\end{frame}

\begin{frame}{Best Subset Selection}
\protect\hypertarget{best-subset-selection}{}
Best subset selection involves fitting a least squares regression for
every possible combination of the \(p\) predictors and then choosing the
best model among them all. It works as follows:

\begin{enumerate}
\tightlist
\item
  For \(k = 1,2, \dots, p\):
\end{enumerate}

\begin{itemize}
\item
  \alert{Fit all possible models that contain $k$ predictors.} (i.e.~if
  \(k = 2\) fit a separate model for each possible combination of two
  predictors)
\item
  \alert{Pick the best model from the set of models with $k$ predictors}
  and call it \(\mathcal{M}_k\). The best model is the one with the
  smallest residual sum of squares (RSS) or largest \(R^2\).
\end{itemize}

\begin{enumerate}
\setcounter{enumi}{1}
\tightlist
\item
  \alert{Select the best model} from the set of
  \(\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}\) where \(\mathcal{M}_{0}\)
  contains no predictors and simply predicts the sample mean for each
  observation.
\end{enumerate}

The methods used to choose the best model in step 2 will be discussed
later on.
\end{frame}

\begin{frame}{Best Subset Selection}
\protect\hypertarget{best-subset-selection-1}{}
There are several difficulties associated with best subset selection.

\begin{itemize}
\item
  The number of models to be fit grows a lot with each increase in
  \(p\).
\item
  It can be very computationally expensive as a result.
\end{itemize}
\end{frame}

\begin{frame}{Forward Stepwise Selection}
\protect\hypertarget{forward-stepwise-selection}{}
Forward stepwise selection works by fitting models with progressively
more predictors, adding the predictors into the model in the order of
greatest model improvement. The process is as follows:

\begin{enumerate}
\item
  Let \(\mathcal{M}_0\) denote the model that contains no predictors.
\item
  For \(k = 0, 1, \dots, p-1\):
\end{enumerate}

\begin{itemize}
\item
  Fit all models that include all the predictors in \(\mathcal{M}_k\)
  along with one additional predictor.
\item
  Choose the best model from the \(p-k\) models above and call it
  \(\mathcal{M}_{k+1}\). The best model is the one with the smallest RSS
  or highest \(R^2\).
\end{itemize}

\begin{enumerate}
\setcounter{enumi}{2}
\tightlist
\item
  Select the best model from
  \(\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}\).
\end{enumerate}
\end{frame}

\begin{frame}{Backward Stepwise Selection}
\protect\hypertarget{backward-stepwise-selection}{}
Backward stepwise selection is simply the reverse of forward stepwise
selection.

\begin{enumerate}
\item
  Let \(\mathcal{M}_p\) be the model that contains all \(p\) predictors.
\item
  For \(k = p,\ p-1, \dots,\ 1:\)
\end{enumerate}

\begin{itemize}
\item
  Fit all models that contain all but one of the predictors in
  \(\mathcal{M}_k\).
\item
  Choose the best from the \(k\) models and call it
  \(\mathcal{M}_{k-1}\). The best model is the one with the smallest RSS
  or highest \(R^2\).
\end{itemize}

\begin{enumerate}
\setcounter{enumi}{2}
\tightlist
\item
  Select the best model from
  \(\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}\).
\end{enumerate}
\end{frame}

\begin{frame}{Exercises: Subset Selection}
\protect\hypertarget{exercises-subset-selection}{}
Open the Linear Model Selection and Regularization R Markdown file.

\begin{itemize}
\item
  Go over the ``Getting Started'' section together as a class.
\item
  Go over the ``Best Subset Selection'' section together as a class.
\item
  Go over the ``Stepwise Selection'' section together as a class.
\end{itemize}
\end{frame}

\begin{frame}{Choosing the Optimal Model}
\protect\hypertarget{choosing-the-optimal-model}{}
Each of the subset selection methods require choosing the best model
from a set of models. We cannot use RSS or \(R^2\) for our final
decision since they will always recommend the model containing all the
predictors since it will have the lowest training error rate.

We need to estimate the test error. There are two approaches:

\begin{itemize}
\item
  \alert{Indirectly estimate the test error} by adjusting the training
  error to include a measure of bias due to overfitting.
\item
  \alert{Directly estimate the test error} using a a validation set
  approach or cross-validation.
\end{itemize}
\end{frame}

\begin{frame}{Indirect Test Error Estimation}
\protect\hypertarget{indirect-test-error-estimation}{}
We will consider four approaches used to adjust the training error.

\begin{itemize}
\item
  \(C_p\)
\item
  Akaike information criterion (AIC)
\item
  Bayesian information criterion (BIC)
\item
  Adjusted \(R^2\)
\end{itemize}
\end{frame}

\begin{frame}{\(C_p\)}
\protect\hypertarget{c_p}{}
Given a fitted least squares model with \(d\) predictors, the \(C_p\)
estimates the test MSE using \[
C_{p}=\frac{1}{n}\left(\mathrm{RSS}+2 d \hat{\sigma}^{2}\right)
\]

\begin{itemize}
\item
  \(\hat{\sigma}^{2}\) is an estimate of the variance of the error for
  each response measurement.
\item
  \(2 d \hat{\sigma}^{2}\) acts as a penalty term for the training RSS
  since it will underestimate the test error.
\item
  The more predictors, the larger the penalty term.
\item
  Choose the model with the lowest \(C_p\)
\end{itemize}
\end{frame}

\begin{frame}{AIC}
\protect\hypertarget{aic}{}
The AIC is usually used for models fit with maxiumum likelihood but in
the case of a linear model with Gaussian errors, maximum likelihood and
least squares are the same thing.
\[\mathrm{AIC}=\frac{1}{n}\left(\mathrm{RSS}+2 d \hat{\sigma}^{2}\right)\]
So for least squares models, \(C_p\) and AIC are the same. As a result,
we choose the model with the lowest AIC value.
\end{frame}

\begin{frame}{BIC}
\protect\hypertarget{bic}{}
Given a fitted least squares model with \(d\) predictors, the BIC is \[
\mathrm{BIC}=\frac{1}{n}\left(\mathrm{RSS}+\log (n) d \hat{\sigma}^{2}\right)
\] - \(n\) is the number of observations

\begin{itemize}
\item
  The BIC places a heavier penalty on models with many variables.
\item
  We choose the model with the lowest BIC.
\end{itemize}
\end{frame}

\begin{frame}{Adjusted \(R^2\)}
\protect\hypertarget{adjusted-r2}{}
The adjusted \(R^2\) statistic is the computed with \[
\text { Adjusted } R^{2}=1-\frac{\mathrm{RSS} /(n-d-1)}{\operatorname{TSS} /(n-1)}\]

\begin{itemize}
\item
  Total sum of squares (TSS) = \(\sum\left(y_{i}-\bar{y}\right)^{2}\)
\item
  We choose the model with the largest adjusted \(R^2\)
\end{itemize}
\end{frame}

\begin{frame}{Exercises: Indirect Error Estimation}
\protect\hypertarget{exercises-indirect-error-estimation}{}
Open the Linear Model Selection and Regularization R Markdown file.

\begin{itemize}
\item
  Go over the section 2.3 together as a class.
\item
  10 minutes for students to complete the questions from section 2.3.
\item
  Questions should be completed at home if time does not allow.
\end{itemize}
\end{frame}

\begin{frame}{Direct Test Error Estimation}
\protect\hypertarget{direct-test-error-estimation}{}
We can compute the validation set error or the cross-validation error
for each model we are attempting to decide between and choose the one
with the smallest test error estimate.

If there are several models that have similar test errors, use the
\textbf{one-standard-error rule}:

\begin{enumerate}
\item
  Calculate the standard error of the estimated test MSE for each model.
\item
  Select the smallest model that has an estimated test error within one
  SE of the smallest MSE of all the models.
\end{enumerate}
\end{frame}

\begin{frame}{Indirect vs Direct Test Error Estimation}
\protect\hypertarget{indirect-vs-direct-test-error-estimation}{}
\begin{itemize}
\item
  Advantages of direct test error estimation via the validation set
  approach or cross-validation:

  \begin{itemize}
  \item
    provides a direct estimate of the test error
  \item
    makes fewer assumptions about the true model
  \end{itemize}
\item
  Advantages of indirect test error estimation via AIC, BIC, \(C_p\),
  and adjusted \(R^2\):

  \begin{itemize}
  \tightlist
  \item
    less computation time for problems with large \(p\) or \(n\)
    (however, with the fast computers nowadays this is hardly ever an
    issue with cross-validation)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Exercises: Direct Error Estimation}
\protect\hypertarget{exercises-direct-error-estimation}{}
Open the Linear Model Selection and Regularization R Markdown file.

\begin{itemize}
\item
  Go over the section 2.4 together as a class.
\item
  10 minutes for students to complete the questions from section 2.4.
\item
  Questions should be completed at home if time does not allow.
\end{itemize}
\end{frame}

\begin{frame}{Shrinkage Methods}
\protect\hypertarget{shrinkage-methods}{}
Shrinkage methods fit a model using a technique that regularizes the
coefficient estimates. This results in shrinking the coefficients
towards zero and thereby reducing their variance.

The two shrinkage methods we will cover are:

\begin{itemize}
\item
  Ridge regression
\item
  Lasso
\end{itemize}
\end{frame}

\begin{frame}{Ridge Regression}
\protect\hypertarget{ridge-regression}{}
Ridge regression works the same as the least squares method except
instead of minimizing the RSS to find estimates of
\(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\), we minimize \[
\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}
\]

\begin{itemize}
\item
  \(\lambda \sum_{j} \beta_{j}^{2}\) is a shrinkage penalty that shrinks
  the estimates of \(\beta_j\) towards zero.
\item
  \(\lambda \geq 0\) is a tuning parameter that controls the impact of
  the penalty term

  \begin{itemize}
  \item
    When \(\lambda = 0 \Rightarrow\) ridge regression = least squares.
  \item
    When \(\lambda \rightarrow \infty \Rightarrow\) ridge regression
    coefficients approach zero.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Ridge Regression}
\protect\hypertarget{ridge-regression-1}{}
There are a few key considerations for ridge regression:

\begin{itemize}
\item
  Different resulting coefficient estimates
  \(\hat{\beta}_{\lambda}^{R}\) for different \(\lambda\).
\item
  Shrinkage penalty is applied to \(\beta_{1}, \ldots, \beta_{p}\) but
  not \(\beta_0\).
\item
  Ridge regression coefficient estimates are not scale equivalent.

  \begin{itemize}
  \tightlist
  \item
    Always standardize the predictors so that they are on the same scale
    before performing ride regression
    \[\tilde{x}_{i j}=\frac{x_{i j}}{\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)^{2}}}\]
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Lasso}
\protect\hypertarget{lasso}{}
The Lasso, like ridge regression, minimizes a different quantity than
least squares regression in order to estimate the regression
coefficients.
\[\mathrm{RSS}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\]

\begin{itemize}
\item
  \(\beta_j^2\) in ridge regression is replaced by
  \(\left|\beta_{j}\right|\) in the lasso.
\item
  The penalty forces some coefficient estimates to be zero for
  sufficiently large \(\lambda\).
\end{itemize}
\end{frame}

\begin{frame}{Ride Regression vs The Lasso}
\protect\hypertarget{ride-regression-vs-the-lasso}{}
The advantages of ridge regression and the lasso over the standard least
squares method are related to the bias-variance trade-off.

\begin{itemize}
\item
  As \(\lambda\) increases, the flexibility of the regression fit
  decreases

  \begin{itemize}
  \item
    decrease in variance
  \item
    increase in bias
  \item
    The test MSE is closely related to variance + bias\(^2\)
  \end{itemize}
\item
  Ridge regression and the lasso work best when least squares estimates
  have high variance.

  \begin{itemize}
  \tightlist
  \item
    when \(p\) is almost as large or larger than \(n\), ridge regression
    and the lasso can still perform well by trading off a small increase
    in bias with a large decrease in variance.
  \end{itemize}
\item
  Both method are substantially less computationally expensive compared
  to subset selection
\end{itemize}
\end{frame}

\begin{frame}{Selecting the Tuning Parameter}
\protect\hypertarget{selecting-the-tuning-parameter}{}
Cross-validation offers a simple way to choose the best tuning parameter
for ridge regression and the lasso.

\begin{itemize}
\item
  Take many \(\lambda\) values.
\item
  Compute the cross-validation error of the fitted model for each
  \(\lambda\).
\item
  Choose the tuning parameter that gives the smallest cross-validation
  error.
\item
  Refit the model using the chosen tuning parameter and the complete set
  of observations.
\end{itemize}
\end{frame}

\begin{frame}{Exercises: Ridge Regression \& The Lasso\}}
\protect\hypertarget{exercises-ridge-regression-the-lasso}{}
Open the Linear Model Selection and Regularization R Markdown file.

\begin{itemize}
\item
  Go over the ``Ridge Regression'' section together as a class.
\item
  Go over the ``The Lasso'' section together as a class.
\item
  10 minutes for students to complete the questions.
\item
  Questions should be completed at home if time does not allow.
\end{itemize}
\end{frame}

\begin{frame}{References}
\protect\hypertarget{references}{}
Chapter 6 of the ISLR2 book:

James, Gareth, et al.~``Linear Model Selection and Regularization.'' An
Introduction to Statistical Learning: with Applications in R, 2nd ed.,
Springer, 2021.
\end{frame}



\end{document}
