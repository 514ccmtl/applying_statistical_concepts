---
title: "Assignment 3"
output:
  html_document: default
  pdf_document: default
---

This assignment is due on __Sunday 14 March__, by midnight. It pertains to content taught in classes 5-7.

This assignment should be completed in R, and a PDF file should be submitted, containing both code and written answers. If you like, you may create your own .Rmd file from scratch, but it is likely easier to modify this one.

As before, questions that require identification and/or interpretation will not penalized for brevity of response: if a question can be answered with 'yes/no', or a numeric value, you may simply state as much. If you incorporate code from the internet (which is not required and generally not advisable), please cite the source within your code (providing a URL is sufficient).

If you like, you may collaborate with others in the class. If you choose to do so, please indicate with whom you have worked at the top of your PDF. Separate submissions are required.

Any questions can be addressed to Navona ([navona.calarco@mail.utoronto.ca]()) and/or Julia ([julia.gallucci@mail.utoronto.ca]()) before the due-date. Please email your submissions to Julia, with the subject `DSI: Assignment 3, Name`.

<mark>__MARKING KEY:__  
__Q1: /12__  
__Q2: /20__  
__Q3: /18__   
__TOTAL: / 50 POINTS__  
__BONUS: Possible +5__</mark>

-----


<u>__Question 1__: Data cleaning and visualization</u>

For this assignment, we'll use the in-built dataset `credit` from `ISLR2` library. 

```{r}
#library
library(ISLR2)

#bring in data, as my_df
my_df <- Credit
```

Before modelling, it's essential to "get a feel" for our data. Use function(s) of your choice (e.g., `summary`, `str`, `?Credit`) to answer the following questions:

_(i)_ [**1 POINT**] How many variables (columns) are there?  

_(ii)_ [**1 POINT**] How many observations (rows) are there?  

_(iii)_ [**1 POINT**] How many factor variables are there?  

_(iv)_ [**1 POINT**] What are the factor labels (English) and levels (value) of the `Married` variable?

_(v)_ [**1 POINT**] What is the range of the `Cards` variable? (Hint: `range` function, and `$`.). 

_(vi)_ [**1 POINT**] It is very important to understand if our data has missing values, which R represents as `NA`. Below, show that there are 0 NA values in the dataset. (Hint: you can use the function `is.na` to search for `NA` values, and wrap that with the `sum` function, to provide a total count.)

```{r}
#code here
```

It is also very important to visualize our data before modelling. The `pairs` function visualizes the pair-wise correlations between all variables.

```{r}
pairs(my_df)
```

Reviewing the image above, answer:  
_(vii)_ [**1 POINT**] Which variable pair looks as if it has the strongest correlation?    

_(viii)_ [**1 POINT**] Name a variable pair that looks as though it has no/little correlation (many correct answers).   

_(ix)_ [**1 POINT**] Why do correlation pairs including the `Region` variable have three columns/rows of points?    

_(x)_ [**3 POINTS**] For much of our modelling, we'll make use of a separate training and testing set. Choose your favourite method to split `my_df` into equally-sized training and testing sets. For clarity, call your training set `train`, and your testing set `test`. 
```{r}
#code here
```

Congrats! Now that our data is familiar and 'clean', let's turn to modelling.

-----

<u>__Question 2__: Regularization via Shrinkage</u>

Shrinkage methods can "extend" or improve upon linear model fits, by pushing coefficients towards (ridge regression) or to zero (lasso), and thus reducing variance. Let's perform ridge regression, using the `glmnet` function from the `glmnet` library. 

```{r, message=FALSE}
#load library
library(glmnet)
```

Use our `my_df` dataset (deriving from Credit). Let's use `Balance` as the response variable, and all other variables as predictors. 

_(i)_ [**2 POINTS**] A necessary first step is to get our data into the format expected by `glmnet`. Specifically, we must provide predictor variables in a matrix, and the response variable in a vector. For clarity, call the predictor matrix `x`, and the response vector `y`. (Hint: your `x` matrix should have should 400 rows and 11 columns. Verify that this is true, using in-built functions of your choice, e.g., `ncol`, `nrow`, or `dim`).  

```{r}
#create a matrix of the predictor variables 

#create a vector of the response variable
```

Let's check out how `model.matrix` has transformed our data. Compare the names of variables in our matrix `x`, compared to `my_df` (hint: use the `colnames` function), and answer:

_(ii)_ [**1 POINT**] Which "type" of variables (numeric, character, factor, etc.) have a new name in `x`?  

_(iii)_ [**1 POINT**] Which variable in `x` has two columns dedicated to it? Why? 

_(iv)_ [**1 POINT**] What variable in `my_df` is missing in x? Why might this be?    

Now that we understand how our data is represented, we can move on to modelling. Fit a ridge regression model, using `glmnet`. (Hint: remember to set the alpha value!)
```{r}
#code here
```

_(v)_ [**4 POINTS**] An essential part of ridge regression (and shrinkage methods more broadly) is to identify an 'ideal' lambda value. Use the appropriate function from `glmnet` to identify this lambda value via cross-validation. (Hint: remember that `x` and `y` should not consist of the complete dataset!)
```{r warning=FALSE}
#code here
```

_(vi)_ [**1 POINT**] By default, cross validation via `glmnet` considers n=100 lambda values. The cross-validated model object that you created in the step above stores these n=100 lambda values within it. Print them here (Hint: use the `$` to "look inside" your model.)
```{r}
#code here
```

_(vii)_ [**1 POINTS**] Visualize your cross-validation results using `plot`. 
```{r}
#code here
```

_(viii)_ [**1 POINTS**] Now, look inside your cross-validated object to pull out the lambda value with the smallest error (Hint: the value will be that shown by the first, left-most vertical dotted line.)
```{r}
#code here
```

_(ix)_ [**1 POINTS**] In your plot, what does the second (right-most) vertical dotted represent? (Hint: read `cv.glmnet`'s help documentation pertaining to `lambda.1se`.). 

_(x)_ [**3 POINTS**] We can now refit ridge regression, for the entire dataset, with the ideal lambda value. Use the lambda value with the smallest error. Provide an argument to print the estimated coefficients (Hint: check out the `type` argument.
```{r}
#code here
```

_(xi)_ [**2 POINTS**] Did you expect any coefficients to be exactly 0? Why or why not?  

_(xii)_ [**2 POINTS**] The plot created above shows that the ideal 'tuning' (penalty) provided by lambda is comparatively small (one of the smallest considered by `glmnet`, if not the smallest). What might this suggest? In your answer, consider the nature of the `Credit` dataset.

-----

<u>__Question 2__ Decision (regression) tree</u>.

Decision trees partition a dataset into smaller subgroups, and then fit a constant for every observation in a given subgroup. This method is well-able to model non-linear associations, and can be helpfully visualized.

Let's continue to work with `my_df` (deriving from the `Credit` dataset). We will use the `tree` library.

```{r}
library(tree)
```

_(i)_ [**2 POINTS**] Below, fit a tree in the training set. Use `Balance` as the response variable, and all other variables as predictors. 
```{r}
#code here
```

_(ii)_ [**2 POINTS**] Plot your tree, with text.
```{r}
#code here
```

Review the plot and/or the model `summary` to answer the following questions:

_(iii)_ [**1 POINT**] What is the most important variable for predicting `Balance` (Hint: what variable is at the top of the tree, i.e., the "root")?  

_(iv)_ [**1 POINT**] How many terminal nodes ("leafs") are there?

_(v)_ [**1 POINT**] What is the tree's error? (Hint: Look for `Residual mean deviance`)

_(vi)_ [**1 POINT**] Imagine an individual with the following characteristics: A `Limit` of $1000, and a `Rating` of 100. What would you predict their `Balance` to be?  

In a sentence or two, answer the following conceptual questions about regression trees:

_(vii)_ [**1 POINT**] Regression trees are created via "recursive binary splitting". Why do we call recursive binary splitting a "top down", or "greedy", approach?  

_(viii)_ [**1 POINT**] Why is "greediness" required?  

_(ix)_ [**1 POINT**] We often "cut" a tree when a given terminal node ("leaf") has fewer than some fixed number of observations (e.g., n=5). Why is this?  

_(x)_ [**1 POINT**] Next, let's determine if our tree would benefit from "pruning". Below, call the appropriate cross-validation function on our tree model.
```{r}
#code here
```

_(xi)_ [**1 POINT**] Plot the cross-validation results.
```{r}
#code here
```

_(xii)_ [**1 POINT**] The cross-validation function operates via k-fold. Review the help documentation this function. How many folds are fit by default?

_(xiii)_ [**2 POINTS**] Does this tree require pruning? How do you know?

Decision trees are often unstable (i.e., they greatly reflect the particular sample upon which they were created; this limitation motivates "ensemble methods", including random forest). 

_(xiv)_ [**2 POINTS**] If we were to fit the same tree model but in a new random sample, where would you expect to see the most variability: the top decision node ("root"), or the terminal nodes ("leafs")? Why? (You can test this if you like, but code is not required.)

-----

<u>__BONUS__: Polynomial regression</u> 

Let's again work with our `my_df`. Again, let's use `Balance` as the response variable, but this time, for ease, let's use only a single variable `Age` as a predictor. 

_(i)_ [**1 POINT**] Fit four models: polynomial models with orders 2, 3, and 4, respectively, as well as the linear model for comparison. 
```{r}
#code here
```

_(ii)_ [**1 POINT**] We will compare these four models, to see which provides the better fit. What is the null hypothesis, $H_0$? What is the alternative hypothesis, $H_1$? 

_(iii)_ [**1 POINT**] Compare these four models, using ANOVA. (Hint: remember, models must be entered in order of complexity).
```{r}
#code here
```
_(iv)_ [**1 POINT**] Based on the ANOVA results, can we reject the null hypothesis?  

_(v)_ [**1 POINT**] Based on the ANOVA results, which model do we say is best?  





